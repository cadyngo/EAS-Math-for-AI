{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cadyngo/EAS-Math-for-AI/blob/main/MatrixFactorization_TODO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Matrix Factorization Methods\n",
        "\n",
        "Now that we have covered the fundamentals of matrix multiplication, let us discuss some important methods in matrix factorization. Firstly, what is matrix factorization? Matrix factorization is a technique in linear algebra that involves breaking down a matrix into a product of other matrices, similar to how factorization works for scalars."
      ],
      "metadata": {
        "id": "hJWrpAlubLPA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: LU Decomposition"
      ],
      "metadata": {
        "id": "RwB9Zra--HQI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LU decomposition with partial pivoting implies the following condition. For a square, nonsingular matrix A $∈\\mathbb{R}^{n\\times n}$, LU with partial pivoting states:\n",
        "\n",
        "$$PA=LU$$\n",
        "\n",
        "where $P$ is the permutation matrix (representing row swaps), $L$ is the unit lower triangular matrix ($diag(L) = 1$), and $U$ is the upper triangular matrix.\n",
        "\n",
        "**We obtain this by performing Gaussian elimination:**\n",
        "\n",
        "* at step $k$, we select a pivot (usually the entry of largest magnitude in column $k$ at or below the diagonal) and optionally swap rows to move it into the $(k, k)$ position (this updates our permutation matrix $P$)\n",
        "\n",
        "* We then subtract multiples of the pivot row from rows below to create zeros beneath the pivot\n",
        "\n",
        "* The subtraction factors for each row $r$ in column $k$ become the subdiagonal entries of $L$ at entry $(r, k)$\n",
        "\n",
        "* Furthermore, what remains after all gaussian transformations on our original matrix $A$ is $U$"
      ],
      "metadata": {
        "id": "knqDgPQa9rHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why is this important for machine learning?**\n",
        "\n",
        "There are many instances where this factorization can be useful.\n",
        "We may want to solve a system $Ax=b$ for many different $b$, a task that is relatively common in machine learning - multi-output regression, for example. Performing LU decomposition once and performing the two triangular solves with $L$ and $U$ is generally computationally cheaper than solving the original system $Ax=b$.\n",
        "\n",
        "**Task:** Implement a function `lu3x3(A)` that identifies and returns $L$ and $U$ for a given $A$. Consider pivoting but don't find the permutation matrix $P$."
      ],
      "metadata": {
        "id": "HiN80_Su-AnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def lu3x3(A):\n",
        "  # TODO: Write solution here\n",
        "\n",
        "  return L, U\n",
        "\n",
        "# Identify L, U\n",
        "A = np.array([[1, 1, 1], [0, 1, 1], [1, 0, 1]])\n",
        "L, U = lu3x3(A)\n",
        "\n",
        "print(\"A:\")\n",
        "print(A)\n",
        "\n",
        "print(\"\\nL:\")\n",
        "print(L)\n",
        "print(\"\\nU:\")\n",
        "print(U)\n",
        "\n",
        "# Verify that our solution works for an arbitrary\n",
        "b = np.array([4, 3, 2])\n",
        "\n",
        "x_org = np.linalg.solve(A, b)\n",
        "x_LU = np.linalg.solve(U, np.linalg.solve(L, b))\n",
        "\n",
        "print(\"\\nOriginal solution:\")\n",
        "print(x_org)\n",
        "\n",
        "print(\"\\nLU solution:\")\n",
        "print(x_LU)\n"
      ],
      "metadata": {
        "id": "KWzoc-7xOByf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7277a78b-be8f-47e9-c3f9-f8ce516eb6d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A:\n",
            "[[1 1 1]\n",
            " [0 1 1]\n",
            " [1 0 1]]\n",
            "\n",
            "L:\n",
            "[[ 1.  0.  0.]\n",
            " [ 0.  1.  0.]\n",
            " [ 1. -1.  1.]]\n",
            "\n",
            "U:\n",
            "[[1 1 1]\n",
            " [0 1 1]\n",
            " [0 0 1]]\n",
            "\n",
            "Original solution:\n",
            "[1. 2. 1.]\n",
            "\n",
            "LU solution:\n",
            "[1. 2. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Singular Value Decomposition (SVD)\n",
        "\n",
        "Singular Value Decomposition (SVD) is a technique to factorize a matrix into 3 different sub-matrices. The specific conditions of SVD are as follows:\n",
        "\n",
        "For any real matrix $A\\in\\mathbb{R}^{m\\times n}$ there exist orthogonal matrices $U\\in\\mathbb{R}^{m\\times m}$ and $V\\in\\mathbb{R}^{n\\times n}$ and a diagonal (rectangular) matrix $\\Sigma\\in\\mathbb{R}^{m\\times n}$ with nonnegative diagonal entries $\\sigma_1\\ge\\cdots\\ge \\sigma_r>0$ (where $r=\\mathrm{rank}(A)$) such that:\n",
        "\n",
        "$$A=UΣV^T,\\quad Σ=\\begin{bmatrix} (diag(σ_1, ...,σ_r)) & 0 \\\\ 0 & 0 \\end{bmatrix}$$"
      ],
      "metadata": {
        "id": "yB_-q7kzcnkz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pseudoinverse**\n",
        "The Moore–Penrose pseudoinverse of $A$ is defined via its SVD.\n",
        "\n",
        "If $A=U\\Sigma V^T$ with $\\Sigma=\\operatorname{diag}(\\sigma_1,\\ldots,\\sigma_r,0,\\ldots)$, then the pseudoinverse is defined as follows $$A^+=VΣ^+U^T$$ where $\\Sigma^+\\in\\mathbb{R}^{n\\times m}$ is formed by taking reciprocals of the nonzero singular values and transposing the rectangular shape: $\\Sigma^+=\\operatorname{diag}(\\frac{1}{\\sigma_1},\\ldots,\\frac{1}{\\sigma_r},0,\\ldots)$.\n",
        "\n",
        "With this, the least–squares solution to $Ax\\approx b$ that minimizes $|Ax-b|_2$ (and among all minimizers, has minimum $|x|_2$) is $x^*=A^+b.$\n",
        "\n",
        "*Useful special cases:*\n",
        "\n",
        "* if $A$ has full column rank ($m\\ge n$, $\\operatorname{rank}(A)=n$) then $(A^TA)^{-1}A^T=A^+$\n",
        "\n",
        "* if $A$ has full row rank ($m\\le n$, $\\operatorname{rank}(A)=m$) then $A^T(AA^T)^{-1}=A^+$\n",
        "  * Geometrically, $AA^+$ projects onto $\\operatorname{col}(A)$ and $A^+A$ projects onto $\\operatorname{row}(A)$"
      ],
      "metadata": {
        "id": "X90PB1VFnJJo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why is this important in machine learning?**\n",
        "\n",
        "SVD underlies several core ideas:\n",
        "\n",
        "* *Dimensionality reduction (PCA)*:\n",
        "  * For a centered data matrix $X\\in\\mathbb{R}^{m\\times n}$ (rows = samples, columns = features), the SVD $X=UΣV^T$ yields principal directions in the columns of $V$\n",
        "  * Truncating to the top $k$ singular values gives a low-dimensional representation $X≈U_kΣ_kV_k^T$ where $U_k\\in\\mathbb{R}^{m\\times k}$, $V_k\\in\\mathbb{R}^{n\\times k}$\n",
        "\n",
        "* *Least squares and stability:*\n",
        "  * Linear regression normal equations can be written with the pseudoinverse: for targets $y$, the minimal-norm LS solution is $\\hat{w} = X^+y$\n",
        "  * Viewing this through the SVD shows which directions (small $\\sigma_i$) are ill-conditioned and amplify noise"
      ],
      "metadata": {
        "id": "eKYwdDG5nMo9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task:** Write `pinv_svd` to compute $A^+=VΣ^+U^T$ and solve on an ill-conditioned system $x^*=A^+b$, and compare this to normal equations.\n",
        "\n",
        "*Note:* To find the SVD, use `np.linalg.svd`, as it is difficult to compute by hand and thus algorithmically"
      ],
      "metadata": {
        "id": "5vEeU5nTnD8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pinv_svd(A, tol=1e-12):\n",
        "  # TODO: Implement SVD inverse here\n",
        "  return\n",
        "\n",
        "# EXAMPLE: Build an ill-conditioned tall system\n",
        "np.random.seed(0)\n",
        "m, n = 60, 15\n",
        "A = np.random.randn(m, n)\n",
        "A[:, 0] = A[:, 1] + 1e-8*np.random.randn(m)\n",
        "\n",
        "# Find ground truth with a little noise\n",
        "x_true = np.random.randn(n)\n",
        "b = A @ x_true + 1e-3*np.random.randn(m)\n",
        "\n",
        "# Solve - SVD\n",
        "x_pinv = pinv_svd(A) @ b\n",
        "\n",
        "# Solve - normal equations/lstsq\n",
        "try:\n",
        "    x_ne = np.linalg.solve(A.T @ A, A.T @ b)\n",
        "except np.linalg.LinAlgError:\n",
        "    x_ne = np.linalg.lstsq(A, b, rcond=None)[0]\n",
        "\n",
        "# Compare errors\n",
        "rel = lambda x: np.linalg.norm(x - x_true)/np.linalg.norm(x_true)\n",
        "print(\"||A x_pinv - b||₂ =\", np.linalg.norm(A @ x_pinv - b))\n",
        "print(\"||A x_ne   - b||₂ =\", np.linalg.norm(A @ x_ne   - b))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNEYe6_xylJR",
        "outputId": "80ffa662-dc25-4482-f90a-a87fe90ef72e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "||A x_pinv - b||₂ = 0.005156966405694077\n",
            "||A x_ne   - b||₂ = 0.005171705003884181\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3: Eigenvalue Decomposition (EVD)\n",
        "\n",
        "For a square matrix $A\\in\\mathbb{R}^{n\\times n}$, an eigenpair $(\\lambda,x\\neq 0)$ satisfies $Ax=\\lambda x$. If $A$ has $n$ linearly independent eigenvectors (i.e., is diagonalizable), then there exist an invertible $X$ and a diagonal $\\Lambda=\\mathrm{diag}(\\lambda_1,\\dots,\\lambda_n)$ such that\n",
        "\n",
        "$$A=XΛX^{-1}\\quad\\text{and}\\quad AX=ΛX$$\n",
        "\n",
        "A particularly important special case is the real symmetric (or complex Hermitian) case, where the spectral theorem guarantees the following orthogonal diagonalization:\n",
        "\n",
        "$$A=A^T ⇒ A=QΛQ^T, \\quad Q^TQ=I, Λ∈\\mathbb{R}^{n\\times n} diagonal$$\n",
        "\n",
        "*When does $A=X\\Lambda X^{-1}$ exist?*\n",
        "\n",
        "* $A$ is diagonalizable over $\\mathbb{C}$ iff the direct sum of eigenspaces has dimension $n$ (equivalently, the minimal polynomial has no repeated root). If not, $A$ is defective and has a Jordan form instead of a diagonal one\n",
        "\n",
        "* Over $\\mathbb{R}$, a real $X\\Lambda X^{-1}$ requires a full set of real eigenvectors"
      ],
      "metadata": {
        "id": "nQC7NmSlNtYZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Useful Special Cases for Finding the EVD computationally**\n",
        "\n",
        "* Symmetric/Hermitian: $A=Q\\Lambda Q^\\top$ (or $Q\\Lambda Q^{!*}$ in complex). Use `np.linalg.eigh` (most accurate; returns real eigenvalues and orthonormal eigenvectors).\n",
        "\n",
        "* Normal matrices (commute with their transpose/unitary adjoint): unitarily diagonalizable; use `eigh` in the Hermitian case.\n",
        "\n",
        "* General (possibly nonsymmetric): use `np.linalg.eig` (eigenvalues/vectors may be complex; eigenvectors need not be orthogonal)."
      ],
      "metadata": {
        "id": "ua5WhhUUPxrX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why is EVD important in machine learning?**\n",
        "\n",
        "* PCA via covariance: For centered data $X\\in\\mathbb{R}^{m\\times d}$, the covariance $C=\\frac{1}{m-1}X^\\top X$ is symmetric PSD. Its EVD $C=Q\\Lambda Q^\\top$ gives principal directions (columns of $Q$) and explained variances (diagonal of $\\Lambda$). (This matches SVD: $X=U\\Sigma V^\\top \\Rightarrow C=V\\Sigma^2V^\\top$.)\n",
        "\n",
        "* Quadratic forms & Rayleigh quotient: For unit $x$, $x^\\top A x\\in[\\lambda_{\\min},\\lambda_{\\max}]$ when $A$ is symmetric. This underlies variance maximization in PCA and many optimization bounds.\n",
        "\n",
        "* Matrix functions & dynamics: If $A=Q\\Lambda Q^\\top$ (symmetric), then $A^k=Q\\Lambda^kQ^\\top$ and $e^{A}=Qe^{\\Lambda}Q^\\top$. This decouples linear systems and graph diffusion."
      ],
      "metadata": {
        "id": "4QZtBYYlRW9H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task**\n",
        "\n",
        "Implement a helper function `evd_symmetric` that calls np.linalg.eigh(A) and verifies $A\\approx Q\\Lambda Q^\\top$ and $Q^\\top Q=I$."
      ],
      "metadata": {
        "id": "T6xRgnpTR_P-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evd_symmetric(A):\n",
        "    # TODO: Implement solution here\n",
        "    return\n",
        "\n",
        "# quick test\n",
        "rng = np.random.default_rng(0)\n",
        "M = rng.normal(size=(6,6)); A = 0.5*(M+M.T)\n",
        "\n",
        "# evaluate reconstruction and orthonormal Q\n",
        "Q, w, ok, ortho = evd_symmetric(A)\n",
        "print(\"reconstruct?\", ok, \"orthonormal Q?\", ortho)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7Oz0ctrSP1M",
        "outputId": "cc65dd84-df4a-47a5-8b63-c5f0232a5a88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reconstruct? True orthonormal Q? True\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}